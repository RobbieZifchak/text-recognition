{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as ks\n",
    "#Preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "#modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten  \n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "#local file nav\n",
    "import os\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import tensorflow as tf\n",
    "from cv2 import imread, imshow\n",
    "\n",
    "np.set_printoptions(threshold=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = '../text-recognition/train'\n",
    "test_directory = '../text-recognition/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11295 images belonging to 56 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_directory,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(64, 64),\n",
    "        class_mode = 'categorical', \n",
    "        batch_size=11295)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1443 images belonging to 56 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator =ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_directory,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(64, 64),\n",
    "        class_mode = 'categorical', \n",
    "        batch_size= 1443)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "# STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "# model.fit_generator(generator=train_generator,\n",
    "#                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     validation_steps=STEP_SIZE_VALID,\n",
    "#                     epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = next(train_generator)\n",
    "test_data, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1443, 64, 64, 1)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1443, 56)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11295, 64, 64, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11295, 56)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the train images \n",
    "train_img_unrow = train_data.reshape(11295, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test images\n",
    "test_img_unrow = test_data.reshape(1443, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 11295)\n",
      "(4096, 1443)\n"
     ]
    }
   ],
   "source": [
    "print(train_img_unrow.shape)\n",
    "print(test_img_unrow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11295, 56)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAEx0lEQVR4nM2WW29cVxXH122ffc6cmbHHlzQ3140VnCahqGmFEKIghMQrz4gv2EfEMyoXFURataUlDlLdlFHsJDiO44zHM3Nue63FAx9hIsX7Ayz9fv+9l/YfZw4LP9mjbvPKWlEIkCM4kpE6AiiSKwMk9g6g7ZrJsEv1pdnezii6Z5gKgiXP0gNwMdMH5cE+2nCl6F+9vE6ICIDuRgiKjYA4uiVf2Ljtvv3Pzn+Lk+3Txxvbl+l4PT96d3mC82r8B4Y6q7IhSbH2w5uCkCxAR5zImUyRQWffd0fhT01/Yt2kUGIP4NEs5MM3H6LM9/+xwHlxluUzH6RJs9UVSAidIGgDJTmRezX+uAKsYZHpmaugg+GcO8BIF0Bh/JeZN2kRrm8fLtqaD2ZlB4E6Je2Ono+uluBG8OrTeTwLidqkLS5UyprzgF7GeHt5gtk8qWdXrn9YpGNdOeLDLRJFpGdnZ99Oio3bq3mkxf53Ky95YtAHGCK+6mj9la3i7jvD1a0LEOIXkwAJ3v7l0O6Ons3eiq+GKw7UTff+3TR1b7z/1o8u49P7lVtqYnnn6mB+iR+fbep3127nm73A4QIozNERB++VQbY2Ry93ebxIOaKePXyCdRttPj2+tfvV2CqZ3b43eDdKyv1OB/IRxF50zWR5AqV8BuslA5fZbkO2rvPCrfHWMVVoYZ7++fJRimkBv7uCKN6jlBGEFowFjfwChBhbaaQ2RxWOscPB8bwflJpADt6JWsr3q546rV7JJHESzzLjROBsQmRvXoEw61AXzqZuIJGlH2ezBINBgKz02tin09Qg0a2EFCwjJyNDFm7NEHT5EPvPVydTVAfQoMAsvVV2ZhkUZNdoOgFaGCkG5p6wI2meULssURAE8OUJlldY36+Q6+ejkQAwKA+HOaE3UBENPto8vf80gRKT15MoqJZyygDABRzBVPILoHB3r+3QvsneWUsk0azUWgROHzyZFW+/Rxv4yTFjCnoWOiMEN0vi1kummSMRvgaCSzv/ImwOTj94f4UDhFQT1ql98VjFN1BwJ7//pD1HDDptPYIjEBijaHDyLtJFeAf9Dw6moFX6bO1WtM5VTKdHD9tnyYqbMXDZbxfjwBDrE3MnoJCSkDOaJ2yJ8M0rULjxq4YzS+effl51STVVLx788ZuaGr93LXCIg/d/0+OpZGV56I5RjADRDcANCsTXUHEkv7tG0iNpP/vr95Nk50++/tsJP1W9cUeETE22ftbPtE98CAxmDhkBgSoEZqbXkMGyAyR5/MUnhqmqB3/f374cde9oYSGf/+DXIwkqKYTsp/Hh0xDxJlsSAHEDR3JX4eQXoSeK0q35o4OWOsQXx2WsajGAlZs/uZRngBpI0+qH7Um+czKIZOrIjkSqSACJYPmvDZNBPX/0+4mhEDZ5mOU9nsffjkY5I6IzdFTVX4837rVXC/YmgKXAWGOWBBDgAvQDTAraVftf1l6cHyYIXfnz66fb63mMmCwDJfTGraYoTMitMyJ4EwEd0OR1rPPSClXwCkPdtK7P/3za2vaNH2cshTEbABCCYXIxNDJBU2QGMCAwRwf05W8BK1YQaDG52unLabVbDpkQGVzQDdAAWzHXDBOFJquL/xOYqzt3xG8+xP8Bi7ioB34n7AgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x142CED050>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_img(train_data[240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[240]\n",
    "#checking that images line up with classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'But': 0,\n",
       " 'Sir': 1,\n",
       " 'a': 2,\n",
       " 'all': 3,\n",
       " 'an': 4,\n",
       " 'and': 5,\n",
       " 'are': 6,\n",
       " 'at': 7,\n",
       " 'be': 8,\n",
       " 'been': 9,\n",
       " 'by': 10,\n",
       " 'can': 11,\n",
       " 'for': 12,\n",
       " 'from': 13,\n",
       " 'had': 14,\n",
       " 'has': 15,\n",
       " 'have': 16,\n",
       " 'her': 17,\n",
       " 'his': 18,\n",
       " 'in': 19,\n",
       " 'into': 20,\n",
       " 'is': 21,\n",
       " 'it': 22,\n",
       " 'last': 23,\n",
       " 'made': 24,\n",
       " 'more': 25,\n",
       " 'no': 26,\n",
       " 'not': 27,\n",
       " 'of': 28,\n",
       " 'on': 29,\n",
       " 'one': 30,\n",
       " 'only': 31,\n",
       " 'or': 32,\n",
       " 'our': 33,\n",
       " 'out': 34,\n",
       " 'people': 35,\n",
       " 'said': 36,\n",
       " 'should': 37,\n",
       " 'so': 38,\n",
       " 'talks': 39,\n",
       " 'than': 40,\n",
       " 'that': 41,\n",
       " 'the': 42,\n",
       " 'their': 43,\n",
       " 'them': 44,\n",
       " 'there': 45,\n",
       " 'this': 46,\n",
       " 'to': 47,\n",
       " 'was': 48,\n",
       " 'were': 49,\n",
       " 'when': 50,\n",
       " 'which': 51,\n",
       " 'who': 52,\n",
       " 'will': 53,\n",
       " 'with': 54,\n",
       " 'would': 55}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as first layer in a sequential model:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(16,)))\n",
    "# now the model will take as input arrays of shape (*, 16)\n",
    "# and output arrays of shape (*, 32)\n",
    "\n",
    "# after the first layer, you don't need to specify\n",
    "# the size of the input anymore:\n",
    "model.add(Dense(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#The first convolution\n",
    "model.add(Convolution2D(16, (3,3), activation='relu', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "# # The second convolution\n",
    "model.add(Convolution2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "# The third convolution\n",
    "model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "#The fourth convolution\n",
    "model.add(Convolution2D(64, (3,3), activation='relu')),\n",
    "model.add(MaxPooling2D(2,2)),\n",
    "# # The fifth convolution\n",
    "\n",
    "model.add(Convolution2D(64, (3,3), activation='relu'),\n",
    "model.add(MaxPooling2D(2,2)),\n",
    "\n",
    "# # # Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "# # 128 neuron in the fully-connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# 5 output neurons for 5 classes with the softmax activation\n",
    "model.add(Dense(56, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'RMSprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11295, 64, 64, 1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11295, 56)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:1000]\n",
    "partial_x_train = train_data[1000:]\n",
    "\n",
    "y_val = train_labels[:1000]\n",
    "partial_y_train = train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11295"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000 + 8295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10295 samples, validate on 1000 samples\n",
      "Epoch 1/24\n",
      "10295/10295 [==============================] - 27s 3ms/step - loss: 0.0764 - accuracy: 0.9737 - val_loss: 0.6674 - val_accuracy: 0.8750\n",
      "Epoch 2/24\n",
      "10295/10295 [==============================] - 24s 2ms/step - loss: 0.0752 - accuracy: 0.9745 - val_loss: 0.6996 - val_accuracy: 0.8610\n",
      "Epoch 3/24\n",
      "10295/10295 [==============================] - 23s 2ms/step - loss: 0.0609 - accuracy: 0.9803 - val_loss: 0.7497 - val_accuracy: 0.8710\n",
      "Epoch 4/24\n",
      "10295/10295 [==============================] - 23s 2ms/step - loss: 0.0576 - accuracy: 0.9807 - val_loss: 0.7512 - val_accuracy: 0.8690\n",
      "Epoch 5/24\n",
      "10295/10295 [==============================] - 24s 2ms/step - loss: 0.0537 - accuracy: 0.9821 - val_loss: 0.7340 - val_accuracy: 0.8720\n",
      "Epoch 6/24\n",
      "10295/10295 [==============================] - 25s 2ms/step - loss: 0.0522 - accuracy: 0.9816 - val_loss: 0.8208 - val_accuracy: 0.8650\n",
      "Epoch 7/24\n",
      "10295/10295 [==============================] - 30s 3ms/step - loss: 0.0436 - accuracy: 0.9852 - val_loss: 0.8467 - val_accuracy: 0.8600\n",
      "Epoch 8/24\n",
      "10295/10295 [==============================] - 27s 3ms/step - loss: 0.0491 - accuracy: 0.9832 - val_loss: 0.8039 - val_accuracy: 0.8700\n",
      "Epoch 9/24\n",
      "10295/10295 [==============================] - 25s 2ms/step - loss: 0.0392 - accuracy: 0.9870 - val_loss: 1.0069 - val_accuracy: 0.8490\n",
      "Epoch 10/24\n",
      "10295/10295 [==============================] - 26s 3ms/step - loss: 0.0392 - accuracy: 0.9871 - val_loss: 0.8760 - val_accuracy: 0.8800\n",
      "Epoch 11/24\n",
      "10295/10295 [==============================] - 26s 3ms/step - loss: 0.0380 - accuracy: 0.9871 - val_loss: 0.8949 - val_accuracy: 0.8650\n",
      "Epoch 12/24\n",
      "10295/10295 [==============================] - 27s 3ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.9710 - val_accuracy: 0.8690\n",
      "Epoch 13/24\n",
      "10295/10295 [==============================] - 25s 2ms/step - loss: 0.0376 - accuracy: 0.9872 - val_loss: 1.2542 - val_accuracy: 0.8440\n",
      "Epoch 14/24\n",
      "10295/10295 [==============================] - 26s 3ms/step - loss: 0.0344 - accuracy: 0.9894 - val_loss: 1.0313 - val_accuracy: 0.8510\n",
      "Epoch 15/24\n",
      "10295/10295 [==============================] - 27s 3ms/step - loss: 0.0332 - accuracy: 0.9900 - val_loss: 0.9541 - val_accuracy: 0.8680\n",
      "Epoch 16/24\n",
      "10295/10295 [==============================] - 25s 2ms/step - loss: 0.0321 - accuracy: 0.9896 - val_loss: 0.8843 - val_accuracy: 0.8820\n",
      "Epoch 17/24\n",
      "10295/10295 [==============================] - 26s 3ms/step - loss: 0.0285 - accuracy: 0.9915 - val_loss: 0.8760 - val_accuracy: 0.8790\n",
      "Epoch 18/24\n",
      "10295/10295 [==============================] - 25s 2ms/step - loss: 0.0333 - accuracy: 0.9908 - val_loss: 0.9507 - val_accuracy: 0.8730\n",
      "Epoch 19/24\n",
      "10295/10295 [==============================] - 26s 2ms/step - loss: 0.0274 - accuracy: 0.9896 - val_loss: 0.9036 - val_accuracy: 0.8790\n",
      "Epoch 20/24\n",
      "10295/10295 [==============================] - 26s 3ms/step - loss: 0.0280 - accuracy: 0.9916 - val_loss: 0.9784 - val_accuracy: 0.8770\n",
      "Epoch 21/24\n",
      "10295/10295 [==============================] - 26s 3ms/step - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.8875 - val_accuracy: 0.8830\n",
      "Epoch 22/24\n",
      "10295/10295 [==============================] - 27s 3ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.8960 - val_accuracy: 0.8730\n",
      "Epoch 23/24\n",
      "10295/10295 [==============================] - 25s 2ms/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 1.0208 - val_accuracy: 0.8800\n",
      "Epoch 24/24\n",
      "10295/10295 [==============================] - 25s 2ms/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 1.1083 - val_accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "baseline_fit = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=24,\n",
    "                    batch_size=50,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = open('../text-recognition/test/it/g01-000-03-02.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread('../text-recognition/test/it/g01-000-03-02.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow('image', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11295, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1443, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12738"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11295 +1443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline ConvNet Architecture example\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional EDA Prep\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
